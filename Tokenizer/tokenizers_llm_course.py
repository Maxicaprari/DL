# -*- coding: utf-8 -*-
"""tokenizers LLM course.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11UhcnJFUSZ2tq2Qq9T9lJebG2-_gFtmx

# TOKENIZER
"""

#pip install transformers

from transformers import AutoTokenizer

sentence = "Hello World"

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokens = tokenizer.tokenize(sentence)
print(tokens)

tokrn_ids =tokenizer.convert_tokens_to_ids(tokens)
print(tokrn_ids)

colors = [
    '102;194;165', '252;141;98', '141;160;203',
    '231;138;195', '166;216;84', '255;217;47'
]

def show_tokens(sentence: str, tokenizer_name: str):


    # Load the tokenizer and tokenize the input
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    token_ids = tokenizer(sentence).input_ids

    # Extract vocabulary length
    print(f"Vocab length: {len(tokenizer)}")

    # Print a colored list of tokens
    for idx, t in enumerate(token_ids):
        print(
            f'\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +
            tokenizer.decode(t) +
            '\x1b[0m',
            end=' '
        )

text = """
English and CAPITALIZATION
üéµ È∏ü
show_tokens False None elif == >= else: two tabs:"    " Three tabs: "       "
12.0*50=600
"""

show_tokens(text, "bert-base-cased")

"""**show_tokens** toma dos valores de entrada: sentence: una oraci√≥n (texto) que se quiere procesar.
tokenizer_name: el nombre de un "tokenizador" que se usar√° para dividir la oraci√≥n en tokens.

Se usa un **for** para recorrer cada token en token_ids.
**idx** es el n√∫mero de posici√≥n del token en la oraci√≥n.
**t** es el identificador num√©rico del token.

**tokenizer.decode(t)** convierte el n√∫mero del token de vuelta en texto.
Se usa **\x1b[...]** para cambiar el color de cada token usando los valores de colors.
üîπ Cada token tendr√° un color diferente basado en su posici√≥n **(idx % len(colors)).**
end=' ' asegura que los tokens se impriman en la misma l√≠nea separados por espacios.
"""

# We can see that the encoder has a lot of problems with the CAPITALIZATION
#UNK is for unknown token

"""# GPT TOKENIZER"""

show_tokens(text, "gpt2")

show_tokens(text, "Xenova/gpt-4")

show_tokens(text, "Qwen/Qwen2-VL-7B-Instruct")

show_tokens(text, "google/flan-t5-small")

show_tokens(text, "microsoft/Phi-3-mini-4k-instruct")

"""Ac√° tenemos una descripcion del modelo, incluidos cantidad de tokens, capas de self attention,activation functions, y hasta la language modelling del final"""